{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8105203b",
   "metadata": {},
   "source": [
    "# Homework 07, Matt Briskey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce3f2e3",
   "metadata": {},
   "source": [
    "### 1. Why the holdout method for model selection suggests to separate the data into three parts: a training set, a validation set, and a test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e35223",
   "metadata": {},
   "source": [
    "In machine learning model selection, we fine tune parameters of different models to further improve the performance for making predictions on unseen data.  If we resued the same test dataset over and over again during model selection, it will become part of our training data and thus the model will be more likely to overfit.  Therefore, it's helpful to split the training data into a training set and a validation set to allow for each model to be tested on previously unseen data and thus reduce the chance of overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7d8a13",
   "metadata": {},
   "source": [
    "### 2. Given a data set (wine), split data (20% test), apply pipeline to standardize the data, classify the data using KNeighborsClassifier (n_neighbors=10), print the test accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcec41ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "df = datasets.load_wine()\n",
    "X = df.data\n",
    "y = df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07f5deb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y,\n",
    "                     test_size=0.20,\n",
    "                     stratify=y,\n",
    "                     random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bfd659b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  94.4%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Create a pipeline with StandardScaler and KNeighborsClassifier\n",
    "pipe_lr = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=10))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = pipe_lr.predict(X_test)\n",
    "\n",
    "# Calculate the test accuracy\n",
    "test_accuracy = pipe_lr.score(X_test, y_test)\n",
    "\n",
    "print(\"Test Accuracy: \", '{:.1%}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbd0156",
   "metadata": {},
   "source": [
    "### 3. What is learning curve? Based on a learning curve, how do you know if the model is over fitting or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251ba392",
   "metadata": {},
   "source": [
    "A learning curve is a graphical representation of the performance of a machine learning model on the training and validation sets. It shows how the model's performance improves or stabilizes as more data is used for training. Learning curves are useful for diagnosing model performance and identifying if the model is overfitting or underfitting.\n",
    "\n",
    "**Overfitting** \\\n",
    "In an overfitting scenario, the model performs well on the training data but poorly on the validation or test data. In underfitting, the learning curve will show a large gap between the training and validation/test performance. The training accuracy will be high, while the validation/test accuracy will be significantly lower and may even plateau or decrease as more data is added. This indicates that the model is memorizing the training data and failing to generalize well to unseen data.\n",
    "\n",
    "**Underfitting** \\\n",
    "In contrast, underfitting occurs when the model performs poorly on both the training and validation/test data. The learning curve will show low accuracy scores for both the training and validation/test sets, with the scores being similar or only slightly improving with more data. This suggests that the model is too simple to capture the underlying patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcd4b9a",
   "metadata": {},
   "source": [
    "### 4. In the above data set, fit KNN using 10-fold cross validation and grid search to optimize the number of neighbors; print the optimized parameters and the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ac32d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'knn__n_neighbors': 5}\n",
      "Best Score: 97.2%\n",
      "Test Accuracy:  94.4%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Load the wine dataset\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline with StandardScaler and KNeighborsClassifier\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'knn__n_neighbors': [1, 3, 5, 7, 9, 11, 13, 15]\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object with the pipeline and parameter grid\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=10)\n",
    "\n",
    "# Fit the grid search on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", '{:.1%}'.format(best_score))\n",
    "\n",
    "# Predict the labels for the test set using the best estimator from grid search\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Calculate the test accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test Accuracy: \", '{:.1%}'.format(test_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8727b6c9",
   "metadata": {},
   "source": [
    "### 5. Calculate the accuracy, precision and recall based on the following confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ae1a8d",
   "metadata": {},
   "source": [
    "Title | Predicted NO | Predicted YES |\n",
    "---| --- | ----------- |\n",
    "Actual NO | 50 | 10 |\n",
    "Actual YES | 5 | 100 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d63bf2",
   "metadata": {},
   "source": [
    "**Accuracy** \\\n",
    "Accuracy = (TP + TN)/(FP + FN + TP + TN) \\\n",
    "Accuracy = (100 + 50) / (100 + 50 + 10 + 5) \\\n",
    "Accuracy = 150/165 \\\n",
    "Accuracy = 90.9%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3b3209",
   "metadata": {},
   "source": [
    "**Precision** \\\n",
    "Precision = TP/(TP + FP) \\\n",
    "Precision = 100 / (100 + 10) \\\n",
    "Precision = 100 / 110 \\\n",
    "Precision = 90.9%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed77eb9",
   "metadata": {},
   "source": [
    "**Recall** \\\n",
    "Recall = TP/(TP + FN) \\\n",
    "Recall = 100 /(100 + 5) \\\n",
    "Recall = 100/105 \\\n",
    "Recall = 95.2%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5af4d5f",
   "metadata": {},
   "source": [
    "### 6. Read the last section in the Chapter 6 of textbook, \"Dealing with class imbalance\". Discuss why the accuracy is not a valid meature metric in an imbalanced dataset? What other metrics can be used then?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21e8b83",
   "metadata": {},
   "source": [
    "Accuracy isn't a valid measure when using an imbalanced dataset because the skewed class distrubtion will naturally lead to high accuracy without the model learning anything about the data.  Additionally, there can be unequal misclassification costs that accuracy doesn't describe.  For these reasons, precision, recall, and area under the ROC curve should be used when the dataset has a class imbalance.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
