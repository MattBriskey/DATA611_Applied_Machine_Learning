{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1a1fa4d",
   "metadata": {},
   "source": [
    "# Homework 10, Applying Machine Learning To Sentiment Analysis\n",
    "# Matt Briskey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fba31a",
   "metadata": {},
   "source": [
    "### 1. Explain the idea of bag-of-words model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1a5484",
   "metadata": {},
   "source": [
    "The bag-of-words model allows us to represent text as numerical feature vectors.  The model involves:\n",
    "\n",
    "1. Tokenization: The first step is to break down the text into individual words or tokens. Punctuation marks and other non-alphanumeric characters are typically removed, and the text is usually converted to lowercase to avoid different representations of the same word due to capitalization.\n",
    "\n",
    "2. Vocabulary construction: After tokenization, a unique vocabulary is constructed by collecting all the distinct words from the entire collection of documents. Each word is treated as a feature.\n",
    "\n",
    "3. Vectorization: To create the numerical representation of a document, a vector is formed, with each element of the vector representing the occurrence or frequency of a word from the vocabulary in the document. \n",
    "\n",
    "This approach ignores the order and strucutre of sentences and instead considers each document as a \"bag\" of words. Bag-of-words modeling is said to be sparase because the unique words in each document represent only a small subset of all the\n",
    "words in the bag-of-words vocabulary, so the feature vectors will mostly consist of zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdbe64c",
   "metadata": {},
   "source": [
    "### 2. What are the two methods to treat the meaningless frequently occurring words?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d23752",
   "metadata": {},
   "source": [
    "1. Term frequency inverse document frequency (TF-IDF) - downweights meaningless frequently occurring words.  TF-IDF is a numerical representation used to reflect the importance of a word in a document relative to a collection of documents (corpus). It takes into account both the frequency of the word in the current document (term frequency) and the rarity of the word in the entire corpus (inverse document frequency).  Words that appear frequently in a document but infrequently across the corpus are often considered more important for determining the document's content or sentiment. Words that occur in many documents across the corpus are usually less informative as they are common across various topics and sentiments.\n",
    "\n",
    "2. Stop word removal - Stop-words are simply those words that are extremely common in all sorts of texts and probably bear no (or only little) useful information that can be used to distinguish between different classes of documents. Examples of stop- words are is, and, has, and like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fa7ecb",
   "metadata": {},
   "source": [
    "### 3. Classify the documents in fetch_20newsgroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fda8e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "categories = ['alt.atheism', 'soc.religion.christian',\n",
    "              'comp.graphics', 'sci.med']\n",
    " \n",
    "X, y = fetch_20newsgroups(categories=categories, shuffle=True, random_state=1, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74dbd9d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: jaeger@buphy.bu.edu (Gregg Jaeger)\\nSubj...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: young@serum.kodak.com (Rich Young)\\nSubj...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: mdw33310@uxa.cso.uiuc.edu (Michael D. Wa...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: Re: Looking for Tseng VESA drivers\\nF...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: cfaks@ux1.cts.eiu.edu (Alice Sanders)\\nS...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2252</th>\n",
       "      <td>From: kmr4@po.CWRU.edu (Keith M. Ryan)\\nSubjec...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2253</th>\n",
       "      <td>From: luis.nobrega@filebank.cts.com (Luis Nobr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2254</th>\n",
       "      <td>From: e_p@unl.edu (edgar pearlstein)\\nSubject:...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2255</th>\n",
       "      <td>From: atterlep@vela.acs.oakland.edu (Cardinal ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2256</th>\n",
       "      <td>From: ls8139@albnyvms.bitnet (larry silverberg...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2257 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  category\n",
       "0     From: jaeger@buphy.bu.edu (Gregg Jaeger)\\nSubj...         0\n",
       "1     From: young@serum.kodak.com (Rich Young)\\nSubj...         2\n",
       "2     From: mdw33310@uxa.cso.uiuc.edu (Michael D. Wa...         3\n",
       "3     Subject: Re: Looking for Tseng VESA drivers\\nF...         1\n",
       "4     From: cfaks@ux1.cts.eiu.edu (Alice Sanders)\\nS...         2\n",
       "...                                                 ...       ...\n",
       "2252  From: kmr4@po.CWRU.edu (Keith M. Ryan)\\nSubjec...         0\n",
       "2253  From: luis.nobrega@filebank.cts.com (Luis Nobr...         1\n",
       "2254  From: e_p@unl.edu (edgar pearlstein)\\nSubject:...         3\n",
       "2255  From: atterlep@vela.acs.oakland.edu (Cardinal ...         3\n",
       "2256  From: ls8139@albnyvms.bitnet (larry silverberg...         2\n",
       "\n",
       "[2257 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe with the data \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'text': X, 'category': y})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3610fbd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text        object\n",
       "category     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the types of the data\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "239ebb82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    599\n",
       "2    594\n",
       "1    584\n",
       "0    480\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a count of each category\n",
    "\n",
    "df.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7b9a6ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: jaeger@buphy.bu.edu (Gregg Jaeger)\\nSubject: Re: The Inimitable Rushdie (Re: An Anecdote about Islam\\nOrganization: Boston University Physics Department\\nLines: 63\\n\\nIn article <1993Apr14.121134.12187@monu6.cc.monash.edu.au> darice@yoyo.cc.monash.edu.au (Fred Rice) writes:\\n\\n>>In article <C5C7Cn.5GB@ra.nrl.navy.mil> khan@itd.itd.nrl.navy.mil (Umar Khan) writes:\\n\\n>I just borrowed a book from the library on Khomeini\\'s fatwa etc.\\n\\n>I found this useful passage regarding the legitimacy of the \"fatwa'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the first raw text before preprocessing \n",
    "\n",
    "df.text[0][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee06e3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set print precision\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f0b97f",
   "metadata": {},
   "source": [
    "### Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f639c9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex to clean HTML, punctuation, etc from the raw text\n",
    "\n",
    "import re\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                           text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) +\n",
    "            ' '.join(emoticons).replace('-', ''))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee4e975c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from jaeger buphy bu edu gregg jaeger subject re the inimitable rushdie re an anecdote about islam organization boston university physics department lines 63 in article darice yoyo cc monash edu au fred rice writes in article khan itd itd nrl navy mil umar khan writes i just borrowed a book from the library on khomeini s fatwa etc i found this useful passage regarding the legitimacy of the fatwa'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the first text after preprocessing\n",
    "\n",
    "preprocessor(df.text[0][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1f72708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Regex preprocessor to clean all of the texts\n",
    "\n",
    "df['text'] = df['text'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d59b85eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>from jaeger buphy bu edu gregg jaeger subject ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>from young serum kodak com rich young subject ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>from mdw33310 uxa cso uiuc edu michael d walke...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject re looking for tseng vesa drivers from...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>from cfaks ux1 cts eiu edu alice sanders subje...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  category\n",
       "0  from jaeger buphy bu edu gregg jaeger subject ...         0\n",
       "1  from young serum kodak com rich young subject ...         2\n",
       "2  from mdw33310 uxa cso uiuc edu michael d walke...         3\n",
       "3  subject re looking for tseng vesa drivers from...         1\n",
       "4  from cfaks ux1 cts eiu edu alice sanders subje...         2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show a sample of the cleaned text\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de8e9f0",
   "metadata": {},
   "source": [
    "### Processing documents into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b380a6af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\16145\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download stopwords\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abffc674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "\n",
    "def tokenizer(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f22332c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords, PorterStemmer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# Define the tokenizer_porter function\n",
    "def tokenizer_porter(data):\n",
    "    return [porter.stem(word) for word in data.split()]\n",
    "\n",
    "# Apply the tokenizer_porter function to each text in the 'text' column\n",
    "tokenized_texts = df['text'].apply(tokenizer_porter)\n",
    "\n",
    "# Remove stopwords from each tokenized text\n",
    "filtered_texts = tokenized_texts.apply(lambda words: [w for w in words if w not in stop])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0308b257",
   "metadata": {},
   "source": [
    "### Training a logistic regression model for document classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9af74905",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Split the data into train and test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['category'], test_size = 0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93637cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Term frequency inverse document frequency (TF-IDF)\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None,\n",
    "                        stop_words='english'\n",
    "                        )\n",
    "\n",
    "param_grid = [{\n",
    "  'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "  'clf__penalty': ['l1', 'l2']},\n",
    "]\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', LogisticRegression(random_state=0, solver='liblinear'))])\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf,\n",
    "                           param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5,\n",
    "                           verbose=1,\n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9eaa3b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Wall time: 22.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vect',\n",
       "                                        TfidfVectorizer(lowercase=False,\n",
       "                                                        stop_words='english')),\n",
       "                                       ('clf',\n",
       "                                        LogisticRegression(random_state=0,\n",
       "                                                           solver='liblinear'))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'clf__penalty': ['l1', 'l2'],\n",
       "                          'vect__tokenizer': [<function tokenizer at 0x00000205451B05E0>,\n",
       "                                              <function tokenizer_porter at 0x00000205451B0790>]}],\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "def7df9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'clf__penalty': 'l2', 'vect__tokenizer': <function tokenizer_porter at 0x00000205451B0790>} \n",
      "CV Accuracy: 0.942\n"
     ]
    }
   ],
   "source": [
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('CV Accuracy: %.3f' % gs_lr_tfidf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff5d05c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.945\n"
     ]
    }
   ],
   "source": [
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print('Test Accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1221b6b6",
   "metadata": {},
   "source": [
    "### Model inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08abcde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = clf.steps[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f92d9105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.94, -0.69, -1.04, -1.4 ]),\n",
       " array([[-0.05, -0.12, -0.07, ..., -0.03, -0.01,  0.03],\n",
       "        [ 0.69,  0.27,  0.08, ..., -0.07, -0.04, -0.01],\n",
       "        [-0.35,  0.01,  0.04, ...,  0.15,  0.07, -0.01],\n",
       "        [-0.31, -0.18, -0.07, ..., -0.03, -0.02, -0.01]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.intercept_, log_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa0bc20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = clf.steps[0][1].vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "973d0fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_series = pd.Series(log_reg.coef_.reshape(-1), name='Coefficients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5133841c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_series = pd.Series(vocab, name='Vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97c65e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       -0.045698\n",
       "1       -0.120636\n",
       "2       -0.071563\n",
       "3       -0.000987\n",
       "4       -0.000237\n",
       "           ...   \n",
       "74583   -0.015923\n",
       "74584   -0.009579\n",
       "74585   -0.030197\n",
       "74586   -0.019083\n",
       "74587   -0.014871\n",
       "Name: Coefficients, Length: 74588, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1d12543",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df = pd.merge(\n",
    "    vocab_series,\n",
    "    coefs_series,\n",
    "    left_on='Vocabulary',\n",
    "    right_index=True\n",
    ").drop('Vocabulary', axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5da7d7a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>graphic</td>\n",
       "      <td>-0.956557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>christ</td>\n",
       "      <td>-0.847417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>ca</td>\n",
       "      <td>-0.809215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>imag</td>\n",
       "      <td>-0.794275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>church</td>\n",
       "      <td>-0.714078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3755</th>\n",
       "      <td>msg</td>\n",
       "      <td>-0.700208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>pitt</td>\n",
       "      <td>-0.697575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2188</th>\n",
       "      <td>sin</td>\n",
       "      <td>-0.697156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>thank</td>\n",
       "      <td>-0.656602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>gordon</td>\n",
       "      <td>-0.650394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  Coefficients\n",
       "418   graphic     -0.956557\n",
       "36     christ     -0.847417\n",
       "360        ca     -0.809215\n",
       "1254     imag     -0.794275\n",
       "78     church     -0.714078\n",
       "3755      msg     -0.700208\n",
       "355      pitt     -0.697575\n",
       "2188      sin     -0.697156\n",
       "88      thank     -0.656602\n",
       "356    gordon     -0.650394"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_df.sort_values('Coefficients').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53cba883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3019</th>\n",
       "      <td>caltech</td>\n",
       "      <td>1.244766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5131</th>\n",
       "      <td>jaeger</td>\n",
       "      <td>1.254913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3170</th>\n",
       "      <td>livesey</td>\n",
       "      <td>1.334555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>t</td>\n",
       "      <td>1.350427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3228</th>\n",
       "      <td>okcforum</td>\n",
       "      <td>1.390929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>atheist</td>\n",
       "      <td>1.978709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>atheism</td>\n",
       "      <td>2.087479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1625</th>\n",
       "      <td>moral</td>\n",
       "      <td>2.315052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5130</th>\n",
       "      <td>islam</td>\n",
       "      <td>2.329774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3017</th>\n",
       "      <td>keith</td>\n",
       "      <td>3.017815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         index  Coefficients\n",
       "3019   caltech      1.244766\n",
       "5131    jaeger      1.254913\n",
       "3170   livesey      1.334555\n",
       "154          t      1.350427\n",
       "3228  okcforum      1.390929\n",
       "467    atheist      1.978709\n",
       "1043   atheism      2.087479\n",
       "1625     moral      2.315052\n",
       "5130     islam      2.329774\n",
       "3017     keith      3.017815"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_df.sort_values('Coefficients').tail(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
