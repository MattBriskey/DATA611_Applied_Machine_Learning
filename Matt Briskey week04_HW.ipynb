{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa2e8d07",
   "metadata": {},
   "source": [
    "# Week 4 Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6175d5b1",
   "metadata": {},
   "source": [
    "# Matt Briskey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca943c4",
   "metadata": {},
   "source": [
    "### Summarize the basic idea of logistic regression, support vector machines (SVM), and decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7effb47",
   "metadata": {},
   "source": [
    "Logistic regression is one of the most widely used algorithms for classification in industry that tries to predict if an input if part of a particular class.  Training data is used to mimize the cost function. Logistics regression is also used to determine the weights of belonging to a particular class (how likely it will rain).\n",
    "\n",
    "upport vector machines aims  to find a hyperplane that separates the data into different classes while maximizing the margin, which is the distance between the hyperplane and the nearest data points of each class.  The objective of using training data is to find the optimal hyperplane by solving an optimization problem.  SVM are an extension of the perceptron.\n",
    "\n",
    "Decision trees breaks the data down into a series of questions and is therefore easy to interpret.  Based on the features in the training set, the decision tree model learns a series of questions to infer the class labels of the samples.  A deicision tree is typically pruned so that it doesn't overfit the data by having too deep of nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38de8211",
   "metadata": {},
   "source": [
    "### Read the section of Random Forest, and summarize the major steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f81fdb",
   "metadata": {},
   "source": [
    "A random forest is a group of decision trees.The idea behind a random forest is to average multiple (deep) decision trees that individually suffer from high variance, therefore building a more robust model that has a better generalization performance and is less susceptible to overfitting.  The general steps involved are:\n",
    "1. Use random sampling with replacement (bootstrapped samples)\n",
    "2. Train decision trees of each of the bootstrapped samples independently.  Randomly selected subsets of the features are used in each bootstrapped sample\n",
    "3. A new data point is passed through each of the bootsrapped trees and the ending classification is jotted down\n",
    "4. The classification with the most votes wins and that classification makes it into the aggregrate tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ddfe43",
   "metadata": {},
   "source": [
    "### Why do we split samples into training and test set? What does the stratify mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8333dac6",
   "metadata": {},
   "source": [
    "Splitting the data between training and test data sets allows for generalization and performance evaluation.  The model needs to perform well on unseen data and the test data is used as a proxy for how well it will perfrom on other unseen data.  Additionally, we need to be able to evaluate the each model on independent data sets that aren't used in the training phase.  Lastly, using test data prevents the model from being overfit to align exclusively to the training data.  If the model performs significantly worse on the test set than on the training set, it suggests overfitting, indicating that the model may not generalize well to new data.\n",
    "\n",
    "Stratifying refers to a technique used to ensure that the class distribution in the dataset is preserved in the training and test sets. When we stratify the data, we aim to maintain the same proportion of classes in both sets as in the original dataset.  Stratification is particularly useful when dealing with imbalanced datasets, where one class may be significantly underrepresented compared to others. By using stratified sampling, we ensure that both the training and test sets have a similar class distribution, which helps prevent biased model performance evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b654a789",
   "metadata": {},
   "source": [
    "### Explain what are the bias-variance trade off and regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab29148c",
   "metadata": {},
   "source": [
    "The bias-variance trade off involves finding the right balance between bias and variance which are two sources of error in a model.  A model with high variance is caused by having too many parameters that lead to a model that is too complex\n",
    "given the underlying data. Similarly, models can also suffer from underfitting (high bias), which means that our model is not complex enough to capture thepattern in the training data well and therefore also suffers from low performance on unseen data.\n",
    "\n",
    "One way of finding a good bias-variance tradeoff is to tune the complexity of the model via regularization. Regularization is a very useful method to handle collinearity (high correlation among features), filter out noise from data, and eventually prevent overfitting. The concept behind regularization is to introduce additional information (bias) to penalize extreme parameter  values. The most common form of regularization is so-called L2 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555bd42c",
   "metadata": {},
   "source": [
    "### When do we use SVM's kernel method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eed6faf",
   "metadata": {},
   "source": [
    "SVM's kernel method can be used for data that is nonlinear.  Kernel methods create nonlinear combinations of the original features to project them onto a higher-dimensional space via a mapping function where it becomes linearly\n",
    "separable (a two dimensional dataset is transformed into a three dimensional dataset that is linearly separable).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f764e116",
   "metadata": {},
   "source": [
    "### Load the scikit-learn's Wine recognition dataset; separate 20% data as test data set; predict the wine quality using the 3 methods (logistic regression, support vector machines, decision tree), and print the accuracy in the test set of each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00c3044b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = datasets.load_wine()\n",
    "\n",
    "X = df.data\n",
    "y = df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d45a5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.423e+01 1.710e+00 2.430e+00 ... 1.040e+00 3.920e+00 1.065e+03]\n",
      " [1.320e+01 1.780e+00 2.140e+00 ... 1.050e+00 3.400e+00 1.050e+03]\n",
      " [1.316e+01 2.360e+00 2.670e+00 ... 1.030e+00 3.170e+00 1.185e+03]\n",
      " ...\n",
      " [1.327e+01 4.280e+00 2.260e+00 ... 5.900e-01 1.560e+00 8.350e+02]\n",
      " [1.317e+01 2.590e+00 2.370e+00 ... 6.000e-01 1.620e+00 8.400e+02]\n",
      " [1.413e+01 4.100e+00 2.740e+00 ... 6.100e-01 1.600e+00 5.600e+02]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf880210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a239f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n"
     ]
    }
   ],
   "source": [
    "print(df.feature_names) # check the feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd48d16b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['class_0' 'class_1' 'class_2']\n"
     ]
    }
   ],
   "source": [
    "print(df.target_names) # list target names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62336a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly split the X and y arrays into 20 percent test data and 80 percent training data \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2, \n",
    "    random_state=2,\n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1cc347e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels counts in y: [59 71 48]\n",
      "Labels counts in y_train: [47 57 38]\n",
      "Labels counts in y_test: [12 14 10]\n"
     ]
    }
   ],
   "source": [
    "print('Labels counts in y:', np.bincount(y))\n",
    "print('Labels counts in y_train:', np.bincount(y_train))\n",
    "print('Labels counts in y_test:', np.bincount(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec60d5b",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "989478b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create an instance of the Logistic Regression model\n",
    "logreg = LogisticRegression (solver='liblinear')\n",
    "\n",
    "# Train the model on the training data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the accuracy\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f45ba5",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2ca07df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.81\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create an instance of the Support Vector Machine model\n",
    "svm = SVC()\n",
    "\n",
    "# Train the model on the training data\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the accuracy\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e6e0bb",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4c56b532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create an instance of the Decision Tree classifier\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "# Train the model on the training data\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the accuracy\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
